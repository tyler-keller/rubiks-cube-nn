{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(3, 3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(3, 5, 3)\n",
    "t, torch.transpose(t, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, torch.transpose(t, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(10, dtype='float32').reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "batch_size = 1\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(1)\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(input, target):\n",
    "    return (input - target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "log_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "    with torch.no_grad():\n",
    "        weight -= weight.grad * learning_rate\n",
    "        bias -= bias.grad * learning_rate\n",
    "        weight.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "    if epoch % log_epochs == 0:\n",
    "        print(f'Epoch {epoch} Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Final Params: {weight.item():.4f}x + {bias.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)\n",
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm)\n",
    "y_pred = model(X_test_norm).detach().numpy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(X_train_norm, y_train, 'o', markersize=10)\n",
    "plt.plot(X_test_norm, y_pred, '--', lw=3)\n",
    "plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)\n",
    "ax.set_xlabel('x', size=15)\n",
    "ax.set_ylabel('y', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if epoch % log_epochs == 0:\n",
    "        print(f'Epoch {epoch} Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1./3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    "model = Model(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# loss_hist = [0] * num_epochs\n",
    "# accuracy_hist = [0] * num_epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     for x_batch, y_batch in train_dl:\n",
    "#         pred = model(x_batch)\n",
    "#         loss = loss_fn(pred, y_batch.long())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
    "#         is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "#         accuracy_hist[epoch] += is_correct.mean()\n",
    "#     loss_hist[epoch] /= len(train_dl.dataset)\n",
    "#     accuracy_hist[epoch] /= len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1./3, random_state=1)\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
    "y_train = torch.from_numpy(y_train) \n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)  \n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return x\n",
    "    \n",
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    " \n",
    "model = Model(input_size, hidden_size, output_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 100\n",
    "loss_hist = [0] * num_epochs\n",
    "accuracy_hist = [0] * num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
    "        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "        accuracy_hist[epoch] += is_correct.sum()\n",
    "        \n",
    "    loss_hist[epoch] /= len(train_dl.dataset)\n",
    "    accuracy_hist[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(loss_hist, lw=3)\n",
    "ax.set_title('Training loss', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(accuracy_hist, lw=3)\n",
    "ax.set_title('Training accuracy', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm).float()\n",
    "y_test = torch.from_numpy(y_test) \n",
    "pred_test = model(X_test_norm)\n",
    "\n",
    "correct = (torch.argmax(pred_test, dim=1) == y_test).float()\n",
    "accuracy = correct.mean()\n",
    " \n",
    "print(f'Test Acc.: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuber as pc\n",
    "from pycuber.solver import CFOPSolver\n",
    "\n",
    "c = pc.Cube()\n",
    "alg = pc.Formula()\n",
    "random_alg = alg.random()\n",
    "\n",
    "print(random_alg)\n",
    "c(random_alg)\n",
    "\n",
    "print(c)\n",
    "solver = CFOPSolver(c)\n",
    "\n",
    "solution = solver.solve(suppress_progress_messages=True)\n",
    "print(solution)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([str(x) for x in np.array(c.get_face('F')).flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.Square('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.Cubie(U=pc.Square('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from src.train import *\n",
    "# # from src.data import *\n",
    "\n",
    "\n",
    "# load_sequences('./data/train_0.dat', num_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_dataset), [20000, 5000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "    \n",
    "print('Vocab-size:', len(token_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(), key=lambda x: x[1], reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([vocab[token] for token in ['this', 'is', 'a', 'sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 2 else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, len_list = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        len_list.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(len_list)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True\n",
    "    )\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3,\n",
    "    padding_idx=0\n",
    ")\n",
    "\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "\n",
    "embedding(text_encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN(64, 32)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randn(5, 3, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (\n",
    "            (pred >= 0.5).float() == label_batch\n",
    "        ).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += (\n",
    "                (pred >= 0.5).float() == label_batch\n",
    "            ).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "         \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# num_epochs = 10 \n",
    "\n",
    "# torch.manual_seed(1)\n",
    " \n",
    "# for epoch in range(num_epochs):\n",
    "#     acc_train, loss_train = train(train_dl)\n",
    "#     acc_valid, loss_valid = evaluate(valid_dl)\n",
    "#     print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nn.Embedding(27, 8)\n",
    "embeddings(torch.Tensor(np.arange(27)).to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuber as pc\n",
    "from pycuber import *\n",
    "\n",
    "def convert_string_state_to_cube(string_state) -> Cube:\n",
    "    '''\n",
    "    Convert the string state to a Pycube object.\n",
    "    ULFRBD ordered. 6 x 9 x 6 state encoding.\n",
    "    '''\n",
    "    cubie_set = set([eval(cubie_string) for cubie_string in string_state.split(';')])\n",
    "    return pc.Cube(cubie_set)\n",
    "\n",
    "\n",
    "def convert_cube_to_state(cube):\n",
    "    '''\n",
    "    Convert a cube object to the NN state representation.\n",
    "    ULFRBD ordered. 6 x 9 x 6 state encoding.\n",
    "    '''\n",
    "    color_vectors = {\n",
    "        'b': np.array([1, 0, 0, 0, 0, 0]),\n",
    "        'g': np.array([0, 1, 0, 0, 0, 0]),\n",
    "        'o': np.array([0, 0, 1, 0, 0, 0]),\n",
    "        'r': np.array([0, 0, 0, 1, 0, 0]),\n",
    "        'w': np.array([0, 0, 0, 0, 1, 0]),\n",
    "        'y': np.array([0, 0, 0, 0, 0, 1]),\n",
    "    }\n",
    "    state = np.zeros(shape=(6, 9, 6))\n",
    "    for i, face in enumerate('ULFRBD'):\n",
    "        unpacked_face = [str(x)[1] for x in np.array(cube.get_face(face)).flatten()]\n",
    "        for j, square in enumerate(unpacked_face):\n",
    "            state[i, j] = color_vectors[square]\n",
    "    return state.flatten()\n",
    "\n",
    "\n",
    "def load_sequences(filename, num_sequences=1000):\n",
    "    '''Load a train.dat file and transform it into a series of cube states to move sequences.\n",
    "    '''\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1, 'U2': 2,\n",
    "        'L': 3, 'L\\'': 4, 'L2': 5,\n",
    "        'F': 6, 'F\\'': 1, 'F2': 8,\n",
    "        'R': 9, 'R\\'': 1, 'R2': 11,\n",
    "        'B': 12, 'B\\'': 1, 'B2': 14,\n",
    "        'D': 15, 'D\\'': 1, 'D2': 17, '$': 18\n",
    "    }\n",
    "    with open(filename, 'r') as f:\n",
    "        print(f'OPENING FILE: {filename}')\n",
    "        log_i = 100\n",
    "        sequences = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i == num_sequences:\n",
    "                return sequences\n",
    "            if i % log_i == 0:\n",
    "                print(f'LINE: {i}')\n",
    "            string_state, solution = line.strip().split('|')\n",
    "            unsolved_cube = convert_string_state_to_cube(string_state)\n",
    "            sequence = []\n",
    "            for step in solution.split():\n",
    "                sequence.append((convert_cube_to_state(unsolved_cube), move_mapping[step]))\n",
    "                unsolved_cube.perform_step(step)\n",
    "            sequence.append((convert_cube_to_state(unsolved_cube), '$'))\n",
    "            sequences.append(sequence)\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubeRNN(nn.Module):\n",
    "    def __init__(self, num_pieces, embedding_dim, hidden_size, output_size, num_layers=1):\n",
    "        super(CubeRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_pieces, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "flattened_cube_state = torch.arange(27)\n",
    "\n",
    "hidden_size = 128\n",
    "output_size = 12 \n",
    "\n",
    "model = CubeRNN(27, 8, hidden_size, output_size)\n",
    "\n",
    "input_tensor = torch.Tensor(np.arange(27)).to(torch.int64)\n",
    "\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(\"Model Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we want to pull one line -- turn it into a sequence of cube states to moves -- then train on it -- then pull the next line and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = pc.Cube()\n",
    "\n",
    "alg = pc.Formula()\n",
    "random_alg = alg.random()\n",
    "cube(random_alg)\n",
    "\n",
    "index = 0\n",
    "for c in cube:\n",
    "    # piece_to_index_mapping[c[1]] = index\n",
    "    # location_to_array_position_mapping\n",
    "    print(c)\n",
    "    # index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do we even need centers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cube(cube, piece_to_index_mapping, location_to_array_position_mapping):\n",
    "    cube_array = [None for _ in range(26)]\n",
    "    for i, cubie_tuple in enumerate(cube):\n",
    "        location, cubie = cubie_tuple\n",
    "        squares = []\n",
    "        for square in cubie:\n",
    "            squares.append(str(square[1]))\n",
    "        squares = tuple(squares)\n",
    "        cube_array[location_to_array_position_mapping[location]] = piece_to_index_mapping[squares]\n",
    "    return torch.Tensor([c for c in cube_array]).to(torch.int64)\n",
    "\n",
    "def encode_move(move):\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1,\n",
    "        'L': 2, 'L\\'': 3,\n",
    "        'F': 4, 'F\\'': 5,\n",
    "        'R': 6, 'R\\'': 7,\n",
    "        'B': 8, 'B\\'': 9,\n",
    "        'D': 10, 'D\\'': 11,\n",
    "        '$': 12\n",
    "    }\n",
    "    return move_mapping[move]\n",
    "\n",
    "def get_sequences(filename, num_sequences=1000):\n",
    "    sequences = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(sequences) == num_sequences:\n",
    "                return sequences\n",
    "            cubies_string, solution_string = line.split('|')\n",
    "            solution_string.translate(str.maketrans({'U2': 'U U', 'L2': 'L L', 'R2': 'R R', 'F2': 'F F', 'D2': 'D D', 'B2': 'B B'}))\n",
    "            cubies_set = set([eval(cubie_string) for cubie_string in cubies_string])\n",
    "            cube = pc.Cube(cubies_set)\n",
    "            moves = solution_string.split()\n",
    "            cube_states = []\n",
    "            move_states = []\n",
    "            for move in moves:\n",
    "                cube_states.append(encode_cube(cube))\n",
    "                move_states.append(encode_move(move))\n",
    "                cube.perform_step(move)\n",
    "            cube_states.append(encode_cube(cube))\n",
    "            move_states.append(encode_move('$'))\n",
    "            sequences.append(cube_states, move_states)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index_mapping = {}\n",
    "location_to_array_position_mapping = {}\n",
    "\n",
    "cube = pc.Cube()\n",
    "index = 0\n",
    "for c in cube:\n",
    "    piece_to_index_mapping[tuple([str(square[1]) for square in c[1]])] = index\n",
    "    location_to_array_position_mapping[c[0]] = index\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_to_array_position_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuber.solver import CFOPSolver\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'CUBE {i}:')\n",
    "    cube = pc.Cube()\n",
    "    alg = pc.Formula()\n",
    "    random_alg = alg.random()\n",
    "    cube(random_alg)\n",
    "    original_cube = cube.copy()\n",
    "    print(f'SCRAMBLED:\\n{encode_cube(cube, piece_to_index_mapping, location_to_array_position_mapping)}')\n",
    "    print(cube)\n",
    "    print()\n",
    "    solver = CFOPSolver(cube)\n",
    "    solution = solver.solve(cube)\n",
    "    print(solution)\n",
    "    original_cube.perform_algo(solution)\n",
    "    print(f'SOLVED:\\n{encode_cube(original_cube, piece_to_index_mapping, location_to_array_position_mapping)}')\n",
    "    print(original_cube)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = get_sequences('data/train_0.dat', num_sequences=100)\n",
    "\n",
    "# num_pieces = 27\n",
    "# embedding_dim = 16 \n",
    "# hidden_size = 128\n",
    "# output_size = 13\n",
    "# num_layers = 1\n",
    "\n",
    "# model = CubeRNN(num_pieces, embedding_dim, hidden_size, output_size, num_layers)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pycuber as pc\n",
    "from pycuber import *\n",
    "\n",
    "def encode_cube(cube):\n",
    "    piece_to_index_mapping = {}\n",
    "    location_to_array_position_mapping = {}\n",
    "    mapping_cube = pc.Cube()\n",
    "    index = 0\n",
    "    for c in mapping_cube:\n",
    "        piece_to_index_mapping[tuple([str(square[1]) for square in c[1]])] = index\n",
    "        location_to_array_position_mapping[c[0]] = index\n",
    "        index += 1\n",
    "    cube_array = [None for _ in range(26)]\n",
    "    for i, cubie_tuple in enumerate(cube):\n",
    "        location, cubie = cubie_tuple\n",
    "        squares = []\n",
    "        for square in cubie:\n",
    "            squares.append(str(square[1]))\n",
    "        squares = tuple(squares)\n",
    "        cube_array[location_to_array_position_mapping[location]] = piece_to_index_mapping[squares]\n",
    "    print(cube_array)\n",
    "    return torch.Tensor([c for c in cube_array]).to(torch.int64)\n",
    "\n",
    "\n",
    "def encode_move(move):\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1,\n",
    "        'L': 2, 'L\\'': 3,\n",
    "        'F': 4, 'F\\'': 5,\n",
    "        'R': 6, 'R\\'': 7,\n",
    "        'B': 8, 'B\\'': 9,\n",
    "        'D': 10, 'D\\'': 11,\n",
    "        '$': 12\n",
    "    }\n",
    "    return move_mapping[move]\n",
    "\n",
    "\n",
    "def random_line(filename, prev_lines):\n",
    "    with open(filename, 'r') as file:\n",
    "        rand_line = next(file)\n",
    "        rand_num = 0\n",
    "        for num, line in enumerate(file, 2):\n",
    "            if random.randrange(num) or num in prev_lines:\n",
    "                continue\n",
    "            rand_line = line\n",
    "            rand_num = num\n",
    "        return rand_num, rand_line\n",
    "\n",
    "\n",
    "def yield_sequences(filename, num_sequences=1000):\n",
    "    i = 0\n",
    "    prev_lines = []\n",
    "    while i < num_sequences:\n",
    "        line_num, line = random_line(filename, prev_lines)\n",
    "        print(line)\n",
    "        prev_lines.append(line_num)\n",
    "        cubies_string, solution_string = line.split('|')\n",
    "        mapping = {'U2': 'U U', 'L2': 'L L', 'R2': 'R R', 'F2': 'F F', 'D2': 'D D', 'B2': 'B B'}\n",
    "        for k, v in mapping.items():\n",
    "            solution_string = solution_string.replace(k, v)\n",
    "        cubies_set = set([eval(cubie_string) for cubie_string in cubies_string.split(';')])\n",
    "        cube = pc.Cube(cubies_set)\n",
    "        print(cube)\n",
    "        moves = solution_string.split()\n",
    "        cube_states = []\n",
    "        move_states = []\n",
    "        for move in moves:\n",
    "            cube_states.append(encode_cube(cube))\n",
    "            move_states.append(encode_move(move))\n",
    "            cube.perform_step(move)\n",
    "        cube_states.append(encode_cube(cube))\n",
    "        move_states.append(encode_move('$'))\n",
    "        if i % 100 == 0:\n",
    "            print(f'Sequnce {i} of {num_sequences}')\n",
    "        i += 1\n",
    "        sequence = cube_states, move_states\n",
    "        print(sequence)\n",
    "        yield sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in yield_sequences('data/train_0.dat', num_sequences=1):\n",
    "    for state in sequence[0]:\n",
    "        print(state.numpy())\n",
    "        print()\n",
    "    print(sequence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycuber.solver import CFOPSolver\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pycuber as pc\n",
    "from pycuber import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def encode_cube(cube):\n",
    "    piece_to_index_mapping = {}\n",
    "    location_to_array_position_mapping = {}\n",
    "    mapping_cube = pc.Cube()\n",
    "    index = 0\n",
    "    for c in mapping_cube:\n",
    "        piece_to_index_mapping[tuple([str(square[1]) for square in c[1]])] = index\n",
    "        location_to_array_position_mapping[c[0]] = index\n",
    "        index += 1\n",
    "    cube_array = [None for _ in range(26)]\n",
    "    for i, cubie_tuple in enumerate(cube):\n",
    "        location, cubie = cubie_tuple\n",
    "        squares = []\n",
    "        for square in cubie:\n",
    "            squares.append(str(square[1]))\n",
    "        squares = tuple(squares)\n",
    "        cube_array[location_to_array_position_mapping[location]] = piece_to_index_mapping[squares]\n",
    "    return torch.Tensor([c for c in cube_array]).to(torch.int64)\n",
    "\n",
    "\n",
    "def encode_move(move):\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1,\n",
    "        'L': 2, 'L\\'': 3,\n",
    "        'F': 4, 'F\\'': 5,\n",
    "        'R': 6, 'R\\'': 7,\n",
    "        'B': 8, 'B\\'': 9,\n",
    "        'D': 10, 'D\\'': 11,\n",
    "        '$': 12\n",
    "    }\n",
    "    return move_mapping[move]\n",
    "\n",
    "\n",
    "def random_line(filename, prev_lines):\n",
    "    with open(filename, 'r') as file:\n",
    "        rand_line = next(file)\n",
    "        rand_num = 0\n",
    "        for num, line in enumerate(file, 2):\n",
    "            if random.randrange(num) or num in prev_lines:\n",
    "                continue\n",
    "            rand_line = line\n",
    "            rand_num = num\n",
    "        return rand_num, rand_line\n",
    "\n",
    "def yield_sequences(filename, num_sequences=1000):\n",
    "    i = 0\n",
    "    prev_lines = []\n",
    "    while i < num_sequences:\n",
    "        line_num, line = random_line(filename, prev_lines)\n",
    "        prev_lines.append(line_num)\n",
    "        cubies_string, solution_string = line.split('|')\n",
    "        mapping = {'U2': 'U U', 'L2': 'L L', 'R2': 'R R', 'F2': 'F F', 'D2': 'D D', 'B2': 'B B'}\n",
    "        for k, v in mapping.items():\n",
    "            solution_string = solution_string.replace(k, v)\n",
    "        cubies_set = set([eval(cubie_string) for cubie_string in cubies_string.split(';')])\n",
    "        cube = pc.Cube(cubies_set)\n",
    "        moves = solution_string.split()\n",
    "        cube_states = []\n",
    "        move_states = []\n",
    "        for move in moves:\n",
    "            cube_states.append(encode_cube(cube))\n",
    "            move_states.append(encode_move(move))\n",
    "            cube.perform_step(move)\n",
    "        cube_states.append(encode_cube(cube))\n",
    "        move_states.append(encode_move('$'))\n",
    "        if i % 100 == 0:\n",
    "            print(f'Sequnce {i} of {num_sequences}')\n",
    "        i += 1\n",
    "        sequence = cube_states, move_states\n",
    "        yield sequence\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer, model, device, num_sequences):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i, sequence in enumerate(yield_sequences('../data/train_0.dat', num_sequences=num_sequences)):\n",
    "        cube_states, move_states = sequence\n",
    "        input_tensor = torch.stack(cube_states).to(device)\n",
    "        output_tensor = torch.Tensor(move_states).to(torch.int64).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_tensor)\n",
    "        loss = loss_fn(outputs, output_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            last_loss = running_loss / num_sequences \n",
    "            print(f'    batch {i} loss: {last_loss}')\n",
    "            tb_x = epoch_index * num_sequences + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            # running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sentence = torch.arange(0, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(sentence.nelement())\n",
    "sentence = sentence.view(-1)[idx].view(sentence.size())\n",
    "# sentence.view(-1)[idx].view(sentence.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.nn.Embedding(10, 16)\n",
    "embedded_sentence = embed(sentence).detach()\n",
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = embedded_sentence.matmul(embedded_sentence.T)\n",
    "omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights = F.softmax(omega, dim=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors = torch.matmul(attention_weights, embedded_sentence)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuber.solver.cfop import CrossSolver, F2LSolver, OLLSolver, PLLSolver, CFOPSolver\n",
    "\n",
    "def remove_double_moves(solution_string):\n",
    "    mapping = {'U2': 'U U', 'L2': 'L L', 'R2': 'R R', 'F2': 'F F', 'D2': 'D D', 'B2': 'B B'}\n",
    "    for k, v in mapping.items():\n",
    "        solution_string = solution_string.replace(k, v)\n",
    "    return solution_string\n",
    "\n",
    "def get_solution(cube: pc.Cube, solver_class) -> str:\n",
    "    original_cube = cube.copy()\n",
    "    solver = solver_class(original_cube)\n",
    "    solution_formula = solver.solve()\n",
    "    solution_string = str(solution_formula)\n",
    "    print(solution_string)\n",
    "    solution_string = remove_double_moves(solution_string)\n",
    "    print(solution_string)\n",
    "    moves = str(solution_string).split()\n",
    "    print(moves)\n",
    "    cube_states = []\n",
    "    move_states = []\n",
    "    for move in moves:\n",
    "        cube_states.append(encode_cube(cube))\n",
    "        move_states.append(encode_move(move))\n",
    "        cube.perform_step(move)\n",
    "    return (cube_states, move_states)\n",
    "\n",
    "\n",
    "def yield_CFOP_sequences(num_sequences=1000):\n",
    "    '''Yields a sequence of cube states to move states.\n",
    "    Stored in a dict w/ 'C', 'F', 'O', 'P' keys denoting what stage of the solve.\n",
    "    '''\n",
    "    i = 0\n",
    "    while i < num_sequences:\n",
    "        cfop_sequence_map = {\n",
    "            'C': None,\n",
    "            'F': None,\n",
    "            'O': None,\n",
    "            'P': None,\n",
    "        }\n",
    "        cfop_solver_map = {\n",
    "            'C': CrossSolver,\n",
    "            'F': F2LSolver,\n",
    "            'O': OLLSolver,\n",
    "            'P': PLLSolver,\n",
    "        } \n",
    "\n",
    "        cube = pc.Cube()\n",
    "\n",
    "        alg = pc.Formula()\n",
    "        scramble = alg.random()\n",
    "        cube(scramble)\n",
    "\n",
    "        for phase, solver in cfop_solver_map.items():\n",
    "            print(phase)\n",
    "            new_cube = cube.copy()\n",
    "            cfop_sequence_map[phase] = get_solution(new_cube, solver)\n",
    "        i += 1\n",
    "        yield cfop_sequence_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "B' R' U D F R2 B2 D'\n",
      "B' R' U D F R R B B D'\n",
      "[\"B'\", \"R'\", 'U', 'D', 'F', 'R', 'R', 'B', 'B', \"D'\"]\n",
      "B'\n",
      "<class 'str'>\n",
      "R'\n",
      "<class 'str'>\n",
      "U\n",
      "<class 'str'>\n",
      "D\n",
      "<class 'str'>\n",
      "F\n",
      "<class 'str'>\n",
      "R\n",
      "<class 'str'>\n",
      "R\n",
      "<class 'str'>\n",
      "B\n",
      "<class 'str'>\n",
      "B\n",
      "<class 'str'>\n",
      "D'\n",
      "<class 'str'>\n",
      "F\n",
      "<generator object F2LSolver.solve at 0x7567c8e47bc0>\n",
      "<generator object F FLSolver.solve at 0x7567c8e47bc0>\n",
      "['<generator', 'object', 'F', 'FLSolver.solve', 'at', '0x7567c8e47bc0>']\n",
      "<generator\n",
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43myield_CFOP_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[97], line 58\u001b[0m, in \u001b[0;36myield_CFOP_sequences\u001b[0;34m(num_sequences)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(phase)\n\u001b[1;32m     57\u001b[0m     new_cube \u001b[38;5;241m=\u001b[39m cube\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 58\u001b[0m     cfop_sequence_map[phase] \u001b[38;5;241m=\u001b[39m \u001b[43mget_solution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_cube\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m cfop_sequence_map\n",
      "Cell \u001b[0;32mIn[97], line 25\u001b[0m, in \u001b[0;36mget_solution\u001b[0;34m(cube, solver_class)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(move))\n\u001b[1;32m     24\u001b[0m     cube_states\u001b[38;5;241m.\u001b[39mappend(encode_cube(cube))\n\u001b[0;32m---> 25\u001b[0m     move_states\u001b[38;5;241m.\u001b[39mappend(\u001b[43mencode_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmove\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m     cube\u001b[38;5;241m.\u001b[39mperform_step(move)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (cube_states, move_states)\n",
      "Cell \u001b[0;32mIn[63], line 47\u001b[0m, in \u001b[0;36mencode_move\u001b[0;34m(move)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_move\u001b[39m(move):\n\u001b[1;32m     38\u001b[0m     move_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m     46\u001b[0m     }\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmove_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmove\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: '<generator'"
     ]
    }
   ],
   "source": [
    "for sequence in yield_CFOP_sequences(num_sequences=1):\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubiks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
