{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(3, 3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(3, 5, 3)\n",
    "t, torch.transpose(t, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, torch.transpose(t, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(10, dtype='float32').reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "batch_size = 1\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(1)\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(input, target):\n",
    "    return (input - target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "log_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "    with torch.no_grad():\n",
    "        weight -= weight.grad * learning_rate\n",
    "        bias -= bias.grad * learning_rate\n",
    "        weight.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "    if epoch % log_epochs == 0:\n",
    "        print(f'Epoch {epoch} Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Final Params: {weight.item():.4f}x + {bias.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)\n",
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm)\n",
    "y_pred = model(X_test_norm).detach().numpy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(X_train_norm, y_train, 'o', markersize=10)\n",
    "plt.plot(X_test_norm, y_pred, '--', lw=3)\n",
    "plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)\n",
    "ax.set_xlabel('x', size=15)\n",
    "ax.set_ylabel('y', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if epoch % log_epochs == 0:\n",
    "        print(f'Epoch {epoch} Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1./3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    "model = Model(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# loss_hist = [0] * num_epochs\n",
    "# accuracy_hist = [0] * num_epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     for x_batch, y_batch in train_dl:\n",
    "#         pred = model(x_batch)\n",
    "#         loss = loss_fn(pred, y_batch.long())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
    "#         is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "#         accuracy_hist[epoch] += is_correct.mean()\n",
    "#     loss_hist[epoch] /= len(train_dl.dataset)\n",
    "#     accuracy_hist[epoch] /= len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1./3, random_state=1)\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
    "y_train = torch.from_numpy(y_train) \n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)  \n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return x\n",
    "    \n",
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    " \n",
    "model = Model(input_size, hidden_size, output_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 100\n",
    "loss_hist = [0] * num_epochs\n",
    "accuracy_hist = [0] * num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
    "        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "        accuracy_hist[epoch] += is_correct.sum()\n",
    "        \n",
    "    loss_hist[epoch] /= len(train_dl.dataset)\n",
    "    accuracy_hist[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(loss_hist, lw=3)\n",
    "ax.set_title('Training loss', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(accuracy_hist, lw=3)\n",
    "ax.set_title('Training accuracy', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm).float()\n",
    "y_test = torch.from_numpy(y_test) \n",
    "pred_test = model(X_test_norm)\n",
    "\n",
    "correct = (torch.argmax(pred_test, dim=1) == y_test).float()\n",
    "accuracy = correct.mean()\n",
    " \n",
    "print(f'Test Acc.: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuber as pc\n",
    "from pycuber.solver import CFOPSolver\n",
    "\n",
    "c = pc.Cube()\n",
    "alg = pc.Formula()\n",
    "random_alg = alg.random()\n",
    "\n",
    "print(random_alg)\n",
    "c(random_alg)\n",
    "\n",
    "print(c)\n",
    "solver = CFOPSolver(c)\n",
    "\n",
    "solution = solver.solve(suppress_progress_messages=True)\n",
    "print(solution)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([str(x) for x in np.array(c.get_face('F')).flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.Square('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.Cubie(U=pc.Square('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from src.train import *\n",
    "# # from src.data import *\n",
    "\n",
    "\n",
    "# load_sequences('./data/train_0.dat', num_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_dataset), [20000, 5000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "    \n",
    "print('Vocab-size:', len(token_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(), key=lambda x: x[1], reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([vocab[token] for token in ['this', 'is', 'a', 'sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 2 else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, len_list = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        len_list.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(len_list)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True\n",
    "    )\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3,\n",
    "    padding_idx=0\n",
    ")\n",
    "\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "\n",
    "embedding(text_encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN(64, 32)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randn(5, 3, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (\n",
    "            (pred >= 0.5).float() == label_batch\n",
    "        ).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += (\n",
    "                (pred >= 0.5).float() == label_batch\n",
    "            ).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "         \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nn.Embedding(27, 8)\n",
    "embeddings(torch.Tensor(np.arange(27)).to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuber as pc\n",
    "from pycuber import *\n",
    "\n",
    "def convert_string_state_to_cube(string_state) -> Cube:\n",
    "    '''\n",
    "    Convert the string state to a Pycube object.\n",
    "    ULFRBD ordered. 6 x 9 x 6 state encoding.\n",
    "    '''\n",
    "    cubie_set = set([eval(cubie_string) for cubie_string in string_state.split(';')])\n",
    "    return pc.Cube(cubie_set)\n",
    "\n",
    "\n",
    "def convert_cube_to_state(cube):\n",
    "    '''\n",
    "    Convert a cube object to the NN state representation.\n",
    "    ULFRBD ordered. 6 x 9 x 6 state encoding.\n",
    "    '''\n",
    "    color_vectors = {\n",
    "        'b': np.array([1, 0, 0, 0, 0, 0]),\n",
    "        'g': np.array([0, 1, 0, 0, 0, 0]),\n",
    "        'o': np.array([0, 0, 1, 0, 0, 0]),\n",
    "        'r': np.array([0, 0, 0, 1, 0, 0]),\n",
    "        'w': np.array([0, 0, 0, 0, 1, 0]),\n",
    "        'y': np.array([0, 0, 0, 0, 0, 1]),\n",
    "    }\n",
    "    state = np.zeros(shape=(6, 9, 6))\n",
    "    for i, face in enumerate('ULFRBD'):\n",
    "        unpacked_face = [str(x)[1] for x in np.array(cube.get_face(face)).flatten()]\n",
    "        for j, square in enumerate(unpacked_face):\n",
    "            state[i, j] = color_vectors[square]\n",
    "    return state.flatten()\n",
    "\n",
    "\n",
    "def load_sequences(filename, num_sequences=1000):\n",
    "    '''Load a train.dat file and transform it into a series of cube states to move sequences.\n",
    "    '''\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1, 'U2': 2,\n",
    "        'L': 3, 'L\\'': 4, 'L2': 5,\n",
    "        'F': 6, 'F\\'': 1, 'F2': 8,\n",
    "        'R': 9, 'R\\'': 1, 'R2': 11,\n",
    "        'B': 12, 'B\\'': 1, 'B2': 14,\n",
    "        'D': 15, 'D\\'': 1, 'D2': 17, '$': 18\n",
    "    }\n",
    "    with open(filename, 'r') as f:\n",
    "        print(f'OPENING FILE: {filename}')\n",
    "        log_i = 100\n",
    "        sequences = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i == num_sequences:\n",
    "                return sequences\n",
    "            if i % log_i == 0:\n",
    "                print(f'LINE: {i}')\n",
    "            string_state, solution = line.strip().split('|')\n",
    "            unsolved_cube = convert_string_state_to_cube(string_state)\n",
    "            sequence = []\n",
    "            for step in solution.split():\n",
    "                sequence.append((convert_cube_to_state(unsolved_cube), move_mapping[step]))\n",
    "                unsolved_cube.perform_step(step)\n",
    "            sequence.append((convert_cube_to_state(unsolved_cube), '$'))\n",
    "            sequences.append(sequence)\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubeRNN(nn.Module):\n",
    "    def __init__(self, num_pieces, embedding_dim, hidden_size, output_size, num_layers=1):\n",
    "        super(CubeRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_pieces, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "flattened_cube_state = torch.arange(27)\n",
    "\n",
    "hidden_size = 128\n",
    "output_size = 12 \n",
    "\n",
    "model = CubeRNN(27, 8, hidden_size, output_size)\n",
    "\n",
    "input_tensor = torch.Tensor(np.arange(27)).to(torch.int64)\n",
    "\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(\"Model Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we want to pull one line -- turn it into a sequence of cube states to moves -- then train on it -- then pull the next line and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = pc.Cube()\n",
    "\n",
    "alg = pc.Formula()\n",
    "random_alg = alg.random()\n",
    "cube(random_alg)\n",
    "\n",
    "index = 0\n",
    "for c in cube:\n",
    "    # piece_to_index_mapping[c[1]] = index\n",
    "    # location_to_array_position_mapping\n",
    "    print(c)\n",
    "    # index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do we even need centers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cube(cube, piece_to_index_mapping, location_to_array_position_mapping):\n",
    "    cube_array = [None for _ in range(26)]\n",
    "    for i, cubie_tuple in enumerate(cube):\n",
    "        location, cubie = cubie_tuple\n",
    "        squares = []\n",
    "        for square in cubie:\n",
    "            squares.append(str(square[1]))\n",
    "        squares = tuple(squares)\n",
    "        cube_array[location_to_array_position_mapping[location]] = piece_to_index_mapping[squares]\n",
    "    return torch.Tensor([c for c in cube_array]).to(torch.int64)\n",
    "\n",
    "def encode_move(move):\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1,\n",
    "        'L': 2, 'L\\'': 3,\n",
    "        'F': 4, 'F\\'': 5,\n",
    "        'R': 6, 'R\\'': 7,\n",
    "        'B': 8, 'B\\'': 9,\n",
    "        'D': 10, 'D\\'': 11,\n",
    "        '$': 12\n",
    "    }\n",
    "    return move_mapping[move]\n",
    "\n",
    "def get_sequences(filename, num_sequences=1000):\n",
    "    log_sequences = 0\n",
    "    sequences = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(sequences) == num_sequences:\n",
    "                return sequences\n",
    "            if len(sequences) % log_sequences == 0:\n",
    "                print(f'Sequence {log_sequences} of {num_sequences}...')\n",
    "            cubies_string, solution_string = line.split('|')\n",
    "            solution_string.translate(str.maketrans({'U2': 'U U', 'L2': 'L L', 'R2': 'R R', 'F2': 'F F', 'D2': 'D D', 'B2': 'B B'}))\n",
    "            cubies_set = set([eval(cubie_string) for cubie_string in cubies_string])\n",
    "            cube = pc.Cube(cubies_set)\n",
    "            moves = solution_string.split()\n",
    "            cube_states = []\n",
    "            move_states = []\n",
    "            for move in moves:\n",
    "                cube_states.append(encode_cube(cube))\n",
    "                move_states.append(encode_move(move))\n",
    "                cube.perform_step(move)\n",
    "            cube_states.append(encode_cube(cube))\n",
    "            move_states.append(encode_move('$'))\n",
    "            sequences.append(cube_states, move_states)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index_mapping = {}\n",
    "location_to_array_position_mapping = {}\n",
    "\n",
    "cube = pc.Cube()\n",
    "index = 0\n",
    "for c in cube:\n",
    "    piece_to_index_mapping[tuple([str(square[1]) for square in c[1]])] = index\n",
    "    location_to_array_position_mapping[c[0]] = index\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_to_array_position_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuber.solver import CFOPSolver\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'CUBE {i}:')\n",
    "    cube = pc.Cube()\n",
    "    alg = pc.Formula()\n",
    "    random_alg = alg.random()\n",
    "    cube(random_alg)\n",
    "    original_cube = cube.copy()\n",
    "    print(f'SCRAMBLED:\\n{encode_cube(cube, piece_to_index_mapping, location_to_array_position_mapping)}')\n",
    "    print(cube)\n",
    "    print()\n",
    "    solver = CFOPSolver(cube)\n",
    "    solution = solver.solve(cube)\n",
    "    print(solution)\n",
    "    original_cube.perform_algo(solution)\n",
    "    print(f'SOLVED:\\n{encode_cube(original_cube, piece_to_index_mapping, location_to_array_position_mapping)}')\n",
    "    print(original_cube)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = get_sequences('data/train_0.dat', num_sequences=100)\n",
    "\n",
    "num_pieces = 27\n",
    "embedding_dim = 16 \n",
    "hidden_size = 128\n",
    "output_size = 13\n",
    "num_layers = 1\n",
    "\n",
    "model = CubeRNN(num_pieces, embedding_dim, hidden_size, output_size, num_layers)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pycuber as pc\n",
    "from pycuber import *\n",
    "\n",
    "def encode_cube(cube):\n",
    "    piece_to_index_mapping = {}\n",
    "    location_to_array_position_mapping = {}\n",
    "    mapping_cube = pc.Cube()\n",
    "    index = 0\n",
    "    for c in mapping_cube:\n",
    "        piece_to_index_mapping[tuple([str(square[1]) for square in c[1]])] = index\n",
    "        location_to_array_position_mapping[c[0]] = index\n",
    "        index += 1\n",
    "    cube_array = [None for _ in range(26)]\n",
    "    for i, cubie_tuple in enumerate(cube):\n",
    "        location, cubie = cubie_tuple\n",
    "        squares = []\n",
    "        for square in cubie:\n",
    "            squares.append(str(square[1]))\n",
    "        squares = tuple(squares)\n",
    "        cube_array[location_to_array_position_mapping[location]] = piece_to_index_mapping[squares]\n",
    "    print(cube_array)\n",
    "    return torch.Tensor([c for c in cube_array]).to(torch.int64)\n",
    "\n",
    "\n",
    "def encode_move(move):\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1,\n",
    "        'L': 2, 'L\\'': 3,\n",
    "        'F': 4, 'F\\'': 5,\n",
    "        'R': 6, 'R\\'': 7,\n",
    "        'B': 8, 'B\\'': 9,\n",
    "        'D': 10, 'D\\'': 11,\n",
    "        '$': 12\n",
    "    }\n",
    "    return move_mapping[move]\n",
    "\n",
    "\n",
    "def random_line(filename, prev_lines):\n",
    "    with open(filename, 'r') as file:\n",
    "        rand_line = next(file)\n",
    "        rand_num = 0\n",
    "        for num, line in enumerate(file, 2):\n",
    "            if random.randrange(num) or num in prev_lines:\n",
    "                continue\n",
    "            rand_line = line\n",
    "            rand_num = num\n",
    "        return rand_num, rand_line\n",
    "\n",
    "\n",
    "def yield_sequences(filename, num_sequences=1000):\n",
    "    i = 0\n",
    "    prev_lines = []\n",
    "    while i < num_sequences:\n",
    "        line_num, line = random_line(filename, prev_lines)\n",
    "        print(line)\n",
    "        prev_lines.append(line_num)\n",
    "        cubies_string, solution_string = line.split('|')\n",
    "        mapping = {'U2': 'U U', 'L2': 'L L', 'R2': 'R R', 'F2': 'F F', 'D2': 'D D', 'B2': 'B B'}\n",
    "        for k, v in mapping.items():\n",
    "            solution_string = solution_string.replace(k, v)\n",
    "        cubies_set = set([eval(cubie_string) for cubie_string in cubies_string.split(';')])\n",
    "        cube = pc.Cube(cubies_set)\n",
    "        print(cube)\n",
    "        moves = solution_string.split()\n",
    "        cube_states = []\n",
    "        move_states = []\n",
    "        for move in moves:\n",
    "            cube_states.append(encode_cube(cube))\n",
    "            move_states.append(encode_move(move))\n",
    "            cube.perform_step(move)\n",
    "        cube_states.append(encode_cube(cube))\n",
    "        move_states.append(encode_move('$'))\n",
    "        if i % 100 == 0:\n",
    "            print(f'Sequnce {i} of {num_sequences}')\n",
    "        i += 1\n",
    "        sequence = cube_states, move_states\n",
    "        print(sequence)\n",
    "        yield sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequnce 0 of 1\n",
      "[ 0  2  3  4  6  7  1  5 16 19 18 13 12 17  8 10 11  9 15 14 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  2  3  4  5  6  7  1 16 19  8 13 12 17 14 18 11  9 15 10 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 7  2  3  0  5  4  6  1 16 19  8 17 12 15 14 13 11  9 18 10 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 7  2  5  3  4  0  6  1 19  8 17 16 12 15 14 13 11  9 18 10 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 7  2  4  5  0  3  6  1  8 17 16 19 12 15 14 13 11  9 18 10 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  7  4  5  0  3  1  2  8 17 16 19 12 15 14 13  9 18 10 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  7  4  5  2  0  3  1  8 17 14 19 12 15 11 16  9 18 10 13 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  7  4  5  1  2  0  3  8 17 11 19 12 15 13 14  9 18 10 16 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  3  7  5  4  2  0  1  8 12 11 19  9 15 17 14 13 18 10 16 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  1  3  5  7  2  0  4  8  9 11 19 13 15 12 14 17 18 10 16 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  1  3  2  7  0  6  4  8  9 11 14 13 19 12 10 17 18 15 16 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 2  1  3  0  7  6  5  4  8  9 11 10 13 14 12 15 17 18 19 16 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 1  4  3  0  7  6  2  5  8  9 11 10 13 14 12 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 1  4  7  3  6  0  2  5  9 11 10  8 13 14 12 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 4  7  3  1  6  0  2  5 14 11 10  8  9 17 12 15 16 13 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 4  7  6  3  0  1  2  5 11 10  8 14  9 17 12 15 16 13 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 4  7  0  6  1  3  2  5 10  8 14 11  9 17 12 15 16 13 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  4  7  0  1  3  2  5  9  8 14 11 13 10 12 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  4  0  3  7  1  2  5 11  9  8 14 13 10 12 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  4  0  3  5  7  1  2 11  9 12 14 13 10 19  8 16 17 18 15 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  4  5  0  7  3  1  2  9 12 14 11 13 10 19  8 16 17 18 15 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  4  5  0  3  1  2  7  9 12  8 11 13 10 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 4  5  0  6  3  1  2  7 10 12  8 11  9 17 14 15 16 13 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 4  5  3  0  1  6  2  7 12  8 11 10  9 17 14 15 16 13 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  5  3  1  6  2  7  9  8 11 10 13 12 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  1  5  6  3  2  7  8 11 10  9 13 12 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  1  5  3  2  7  6  8 11 15  9 13 12 10 19 16 17 18 14 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  3  1  2  5  7  6 11 15  9  8 13 12 10 19 16 17 18 14 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  2  3  5  1  7  6 15  9  8 11 13 12 10 19 16 17 18 14 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  2  3  6  5  1  7 15  9 10 11 13 12 14  8 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  3  5  2  6  1  7 11 15  9 10 13 12 14  8 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  3  5  6  1  7  2 11 15  8 10 13 12  9 19 16 17 18 14 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  5  1  3  6  7  2 10 11 15  8 13 12  9 19 16 17 18 14 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  5  1  2  3  6  7 10 11  9  8 13 12 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  7  4  1  5  3  6  2 10 13  9  8 16 12 11 15 14 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  7  5  4  3  1  6  2 13  9  8 10 16 12 11 15 14 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  5  3  4  2  1  6  7 13 11  8 10  9 12 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  5  2  3  1  4  6  7 11  8 10 13  9 12 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  5  2  0  1  3  4  7 11  8 10 12  9 18 14 13 16 17 15 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  5  0  3  2  1  4  7 12 11  8 10  9 18 14 13 16 17 15 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  5  0  1  2  4  6  7 12 11  8 13  9 10 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  5  2  0  4  1  6  7 11  8 13 12  9 10 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  5  2  3  4  0  1  7 11  8 13 10  9 18 14 12 16 17 15 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  5  3  0  2  4  1  7 10 11  8 13  9 18 14 12 16 17 15 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  5  3  4  2  1  6  7 10 11  8 12  9 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  5  2  3  1  4  6  7 11  8 12 10  9 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  7  5  3  2  4  6  1 11  9 12 10 16 13  8 15 14 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  7  2  5  4  3  6  1  9 12 10 11 16 13  8 15 14 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  7  4  2  3  5  6  1 12 10 11  9 16 13  8 15 14 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  3  2  1  5  6  7 12  8 11  9 10 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  1  3  5  2  6  7  8 11  9 12 10 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  0  4  1  5  2  6  7 10 11  9 12 17  8 14 15 16 13 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  0  1  2  4  5  6  7 12 10 11  9 17  8 14 15 16 13 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8 10 11  9 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  3  5  2  4  6  7  9  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  3  2  5  7  4  6  1  9 14 10 11  8 13 16 15 12 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  3  2  4  7  6  0  1  9 14 10 15  8 11 16 18 12 17 13 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  2  4  5  7  6  0  1 11 14 10 15  9 17 16 18 12  8 13 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  1  2  5  4  6  0  7 11  9 10 15 12 17 14 18 16  8 13 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7 11  9 10 17 12 13 14 15 16  8 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  4  2  5  3  6  7  9 10 17 11 12 13 14 15 16  8 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  5  4  3  2  6  7 10 17 11  9 12 13 14 15 16  8 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  5  3  4  7  2  6  1 10 14 11  9 17 13 16 15 12  8 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 4  5  3  2  7  6  0  1 10 14 11 15 17  9 16 18 12  8 13 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  3  2  4  7  6  0  1  9 14 11 15 10  8 16 18 12 17 13 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  1  3  4  2  6  0  7  9 10 11 15 12  8 14 18 16 17 13 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  3  5  2  4  6  7  9 10 11  8 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7 10 11  8  9 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  7  1  3  2  5  6  4 10 12  8  9 16 13 11 15 14 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  4  7  3  1  5  6  2 10 16  8  9 14 13 12 15 11 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  4  7  5  1  6  0  2 10 16  8 15 14  9 12 18 11 17 13 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  4  7  6  1  0  3  2 10 16  8 18 14 15 12 13 11 17  9 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  5  7  6  1  0  2  4 10 16  8 18 14 15 12 13 17  9 19 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  4  5  6  7  0  2  1 10 14  8 18 17 15 16 13 12  9 19 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 3  1  4  6  5  0  2  7 10 17  8 18 12 15 14 13 16  9 19 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  1  4  0  5  2  3  7 10 17  8 13 12 18 14 19 16  9 15 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  4  2  5  3  6  7 10 17  8 19 12 13 14 15 16  9 18 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  5  4  3  2  6  7 17  8 19 10 12 13 14 15 16  9 18 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  3  5  2  4  6  7  8 19 10 17 12 13 14 15 16  9 18 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  7  1  5  3  4  6  2  8 12 10 17 16 13 19 15 14  9 18 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  2  7  5  1  4  6  3  8 16 10 17 14 13 12 15 19  9 18 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  2  7  4  1  6  0  3  8 16 10 15 14 17 12 18 19  9 13 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 4  2  7  6  1  0  5  3  8 16 10 18 14 15 12 13 19  9 17 11 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  4  7  6  1  0  3  2  8 16 10 18 14 15 12 13  9 17 11 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  2  4  6  7  0  3  1  8 14 10 18  9 15 16 13 12 17 11 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 5  1  2  6  4  0  3  7  8  9 10 18 12 15 14 13 16 17 11 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 6  1  2  0  4  3  5  7  8  9 10 13 12 18 14 11 16 17 15 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "\n",
      "[6, 9, 0, 0, 10, 6, 6, 4, 4, 8, 8, 11, 0, 2, 0, 0, 3, 1, 6, 0, 7, 2, 0, 3, 0, 7, 0, 0, 6, 1, 7, 1, 6, 4, 0, 5, 0, 9, 1, 8, 0, 9, 1, 8, 0, 4, 0, 0, 5, 0, 3, 1, 2, 1, 5, 8, 2, 4, 9, 0, 0, 5, 8, 2, 4, 9, 0, 4, 4, 8, 8, 10, 4, 4, 8, 8, 0, 0, 4, 4, 8, 8, 10, 4, 4, 8, 8, 12]\n"
     ]
    }
   ],
   "source": [
    "for sequence in yield_sequences('data/train_0.dat', num_sequences=1):\n",
    "    for state in sequence[0]:\n",
    "        print(state.numpy())\n",
    "        print()\n",
    "    print(sequence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycuber.solver import CFOPSolver\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pycuber as pc\n",
    "from pycuber import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def encode_cube(cube):\n",
    "    piece_to_index_mapping = {}\n",
    "    location_to_array_position_mapping = {}\n",
    "    mapping_cube = pc.Cube()\n",
    "    index = 0\n",
    "    for c in mapping_cube:\n",
    "        piece_to_index_mapping[tuple([str(square[1]) for square in c[1]])] = index\n",
    "        location_to_array_position_mapping[c[0]] = index\n",
    "        index += 1\n",
    "    cube_array = [None for _ in range(26)]\n",
    "    for i, cubie_tuple in enumerate(cube):\n",
    "        location, cubie = cubie_tuple\n",
    "        squares = []\n",
    "        for square in cubie:\n",
    "            squares.append(str(square[1]))\n",
    "        squares = tuple(squares)\n",
    "        cube_array[location_to_array_position_mapping[location]] = piece_to_index_mapping[squares]\n",
    "    return torch.Tensor([c for c in cube_array]).to(torch.int64)\n",
    "\n",
    "\n",
    "def encode_move(move):\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1,\n",
    "        'L': 2, 'L\\'': 3,\n",
    "        'F': 4, 'F\\'': 5,\n",
    "        'R': 6, 'R\\'': 7,\n",
    "        'B': 8, 'B\\'': 9,\n",
    "        'D': 10, 'D\\'': 11,\n",
    "        '$': 12\n",
    "    }\n",
    "    return move_mapping[move]\n",
    "\n",
    "\n",
    "def random_line(filename, prev_lines):\n",
    "    with open(filename, 'r') as file:\n",
    "        rand_line = next(file)\n",
    "        rand_num = 0\n",
    "        for num, line in enumerate(file, 2):\n",
    "            if random.randrange(num) or num in prev_lines:\n",
    "                continue\n",
    "            rand_line = line\n",
    "            rand_num = num\n",
    "        return rand_num, rand_line\n",
    "\n",
    "def yield_sequences(filename, num_sequences=1000):\n",
    "    i = 0\n",
    "    prev_lines = []\n",
    "    while i < num_sequences:\n",
    "        line_num, line = random_line(filename, prev_lines)\n",
    "        prev_lines.append(line_num)\n",
    "        cubies_string, solution_string = line.split('|')\n",
    "        mapping = {'U2': 'U U', 'L2': 'L L', 'R2': 'R R', 'F2': 'F F', 'D2': 'D D', 'B2': 'B B'}\n",
    "        for k, v in mapping.items():\n",
    "            solution_string = solution_string.replace(k, v)\n",
    "        cubies_set = set([eval(cubie_string) for cubie_string in cubies_string.split(';')])\n",
    "        cube = pc.Cube(cubies_set)\n",
    "        moves = solution_string.split()\n",
    "        cube_states = []\n",
    "        move_states = []\n",
    "        for move in moves:\n",
    "            cube_states.append(encode_cube(cube))\n",
    "            move_states.append(encode_move(move))\n",
    "            cube.perform_step(move)\n",
    "        cube_states.append(encode_cube(cube))\n",
    "        move_states.append(encode_move('$'))\n",
    "        if i % 100 == 0:\n",
    "            print(f'Sequnce {i} of {num_sequences}')\n",
    "        i += 1\n",
    "        sequence = cube_states, move_states\n",
    "        yield sequence\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer, model, device, num_sequences):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i, sequence in enumerate(yield_sequences('../data/train_0.dat', num_sequences=num_sequences)):\n",
    "        cube_states, move_states = sequence\n",
    "        input_tensor = torch.stack(cube_states).to(device)\n",
    "        output_tensor = torch.Tensor(move_states).to(torch.int64).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_tensor)\n",
    "        loss = loss_fn(outputs, output_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            last_loss = running_loss / num_sequences \n",
    "            print(f'    batch {i} loss: {last_loss}')\n",
    "            tb_x = epoch_index * num_sequences + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            # running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sentence = torch.arange(0, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(sentence.nelement())\n",
    "sentence = sentence.view(-1)[idx].view(sentence.size())\n",
    "# sentence.view(-1)[idx].view(sentence.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 3, 2, 7, 0, 6, 1])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = torch.nn.Embedding(10, 16)\n",
    "embedded_sentence = embed(sentence).detach()\n",
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14.2158, -4.9388,  5.5194, -1.7171,  3.4188, -0.9876, -4.2731,  4.8675],\n",
       "        [-4.9388, 12.2017,  0.2491, -0.8745,  1.6346, -1.8930, -2.1571,  3.3948],\n",
       "        [ 5.5194,  0.2491, 11.4957, -3.8354,  2.5251, -0.9206, -4.1464,  4.6170],\n",
       "        [-1.7171, -0.8745, -3.8354, 11.7523,  1.7774,  1.2195,  2.9653, -2.1176],\n",
       "        [ 3.4188,  1.6346,  2.5251,  1.7774, 14.2209, -2.8168, -3.5490,  3.4774],\n",
       "        [-0.9876, -1.8930, -0.9206,  1.2195, -2.8168, 17.7463,  8.8014,  3.6741],\n",
       "        [-4.2731, -2.1571, -4.1464,  2.9653, -3.5490,  8.8014, 20.6077, -2.4939],\n",
       "        [ 4.8675,  3.3948,  4.6170, -2.1176,  3.4774,  3.6741, -2.4939,  8.1024]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega = embedded_sentence.matmul(embedded_sentence.T)\n",
    "omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9972e-01, 4.7989e-09, 1.6714e-04, 1.2031e-07, 2.0454e-05, 2.4953e-07,\n",
       "         9.3378e-09, 8.7086e-05],\n",
       "        [3.5969e-08, 9.9981e-01, 6.4414e-06, 2.0942e-06, 2.5745e-05, 7.5630e-07,\n",
       "         5.8073e-07, 1.4968e-04],\n",
       "        [2.5289e-03, 1.3003e-05, 9.9630e-01, 2.1886e-07, 1.2662e-04, 4.0372e-06,\n",
       "         1.6037e-07, 1.0257e-03],\n",
       "        [1.4132e-06, 3.2820e-06, 1.6991e-07, 9.9977e-01, 4.6540e-05, 2.6640e-05,\n",
       "         1.5267e-04, 9.4683e-07],\n",
       "        [2.0356e-05, 3.4184e-06, 8.3287e-06, 3.9430e-06, 9.9994e-01, 3.9867e-08,\n",
       "         1.9169e-08, 2.1585e-05],\n",
       "        [7.3102e-09, 2.9560e-09, 7.8168e-09, 6.6440e-08, 1.1736e-09, 9.9987e-01,\n",
       "         1.3039e-04, 7.7347e-07],\n",
       "        [1.5646e-11, 1.2982e-10, 1.7760e-11, 2.1777e-08, 3.2275e-11, 7.4576e-06,\n",
       "         9.9999e-01, 9.2709e-11],\n",
       "        [3.5760e-02, 8.2001e-03, 2.7837e-02, 3.3098e-05, 8.9061e-03, 1.0842e-02,\n",
       "         2.2719e-05, 9.0840e-01]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights = F.softmax(omega, dim=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1854, -1.8087, -0.3428,  0.7666,  0.0398, -1.1571,  0.5034,  0.5620,\n",
       "          0.2584, -0.3654, -0.6428, -0.7528,  1.5601,  0.4247,  2.0165,  0.6555],\n",
       "        [-0.3947,  0.0824,  0.5487, -0.9103,  0.6939,  1.2511,  1.9571, -0.4710,\n",
       "         -0.2741, -0.5558,  0.3748,  1.1640, -0.8045,  1.3418, -0.6803, -0.1304],\n",
       "        [-0.4381, -1.8723,  0.0680,  0.8047,  0.2958, -0.8222,  0.5955,  0.2248,\n",
       "          1.0667, -0.6261,  0.3111, -0.0245,  1.0158,  0.6362, -0.4212, -1.6400],\n",
       "        [-0.6785,  0.9241,  1.3744,  0.3274, -0.8578, -0.7545,  0.0116, -1.0264,\n",
       "          0.0519,  1.7652,  0.7899,  0.6979, -0.5031, -0.2519,  1.0975,  0.5728],\n",
       "        [-1.3402, -1.4066,  0.7178, -1.0464,  0.9663, -1.2349,  1.3424, -1.2772,\n",
       "          0.3955,  0.4236, -0.7882, -0.4348,  0.2669, -0.9101, -0.3218,  0.8908],\n",
       "        [-0.6976, -0.0219,  0.3955,  1.0891,  0.6347,  0.7564, -2.0288, -1.0263,\n",
       "          1.0705, -1.1074, -0.3236, -0.4997, -2.1356,  0.2621,  1.5101, -0.3651],\n",
       "        [-1.3361,  2.2292, -0.2377,  1.1522, -0.2701,  1.6111, -1.0301, -2.0469,\n",
       "          0.0530,  0.1297, -0.1023, -1.9736,  0.1933,  0.2815,  0.4892, -0.5179],\n",
       "        [-0.5370, -1.5300, -0.1888,  0.3024,  0.4434,  0.1878,  1.0749, -0.1393,\n",
       "          0.5672, -0.5125, -0.9037,  0.2010, -0.5510,  0.5512,  1.0015, -0.4909]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = torch.matmul(attention_weights, embedded_sentence)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubiks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
