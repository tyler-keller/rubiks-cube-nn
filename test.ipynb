{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(3, 3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(3, 5, 3)\n",
    "t, torch.transpose(t, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, torch.transpose(t, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(10, dtype='float32').reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "batch_size = 1\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(1)\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(input, target):\n",
    "    return (input - target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "log_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "    with torch.no_grad():\n",
    "        weight -= weight.grad * learning_rate\n",
    "        bias -= bias.grad * learning_rate\n",
    "        weight.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "    if epoch % log_epochs == 0:\n",
    "        print(f'Epoch {epoch} Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Final Params: {weight.item():.4f}x + {bias.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)\n",
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm)\n",
    "y_pred = model(X_test_norm).detach().numpy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(X_train_norm, y_train, 'o', markersize=10)\n",
    "plt.plot(X_test_norm, y_pred, '--', lw=3)\n",
    "plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)\n",
    "ax.set_xlabel('x', size=15)\n",
    "ax.set_ylabel('y', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if epoch % log_epochs == 0:\n",
    "        print(f'Epoch {epoch} Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1./3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    "model = Model(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# loss_hist = [0] * num_epochs\n",
    "# accuracy_hist = [0] * num_epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     for x_batch, y_batch in train_dl:\n",
    "#         pred = model(x_batch)\n",
    "#         loss = loss_fn(pred, y_batch.long())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
    "#         is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "#         accuracy_hist[epoch] += is_correct.mean()\n",
    "#     loss_hist[epoch] /= len(train_dl.dataset)\n",
    "#     accuracy_hist[epoch] /= len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1./3, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
    "y_train = torch.from_numpy(y_train) \n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)  \n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return x\n",
    "    \n",
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    " \n",
    "model = Model(input_size, hidden_size, output_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "loss_hist = [0] * num_epochs\n",
    "accuracy_hist = [0] * num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
    "        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "        accuracy_hist[epoch] += is_correct.sum()\n",
    "        \n",
    "    loss_hist[epoch] /= len(train_dl.dataset)\n",
    "    accuracy_hist[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(loss_hist, lw=3)\n",
    "ax.set_title('Training loss', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(accuracy_hist, lw=3)\n",
    "ax.set_title('Training accuracy', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm).float()\n",
    "y_test = torch.from_numpy(y_test) \n",
    "pred_test = model(X_test_norm)\n",
    "\n",
    "correct = (torch.argmax(pred_test, dim=1) == y_test).float()\n",
    "accuracy = correct.mean()\n",
    " \n",
    "print(f'Test Acc.: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pycuber as pc\n",
    "from pycuber.solver import CFOPSolver\n",
    "\n",
    "c = pc.Cube()\n",
    "alg = pc.Formula()\n",
    "random_alg = alg.random()\n",
    "print(random_alg)\n",
    "c(random_alg)\n",
    "print(c)\n",
    "\n",
    "solver = CFOPSolver(c)\n",
    "\n",
    "solution = solver.solve(suppress_progress_messages=True)\n",
    "\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([str(x) for x in np.array(c.get_face('F')).flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.Square('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.Cubie(U=pc.Square('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from src.train import *\n",
    "# # from src.data import *\n",
    "\n",
    "\n",
    "# load_sequences('./data/train_0.dat', num_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_dataset), [20000, 5000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "    \n",
    "print('Vocab-size:', len(token_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(), key=lambda x: x[1], reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([vocab[token] for token in ['this', 'is', 'a', 'sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 2 else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, len_list = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        len_list.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(len_list)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True\n",
    "    )\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3,\n",
    "    padding_idx=0\n",
    ")\n",
    "\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "\n",
    "embedding(text_encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN(64, 32)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randn(5, 3, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=false, batch_first=true\n",
    "        )\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (\n",
    "            (pred >= 0.5).float() == label_batch\n",
    "        ).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += (\n",
    "                (pred >= 0.5).float() == label_batch\n",
    "            ).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "         \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    " \n",
    "# for epoch in range(num_epochs):\n",
    "#     acc_train, loss_train = train(train_dl)\n",
    "#     acc_valid, loss_valid = evaluate(valid_dl)\n",
    "#     print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nn.Embedding(27, 8)\n",
    "embeddings(torch.Tensor(np.arange(27)).to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuber as pc\n",
    "from pycuber import *\n",
    "\n",
    "def convert_string_state_to_cube(string_state) -> Cube:\n",
    "    '''\n",
    "    Convert the string state to a Pycube object.\n",
    "    ULFRBD ordered. 6 x 9 x 6 state encoding.\n",
    "    '''\n",
    "    cubie_set = set([eval(cubie_string) for cubie_string in string_state.split(';')])\n",
    "    return pc.Cube(cubie_set)\n",
    "\n",
    "\n",
    "def convert_cube_to_state(cube):\n",
    "    '''\n",
    "    Convert a cube object to the NN state representation.\n",
    "    ULFRBD ordered. 6 x 9 x 6 state encoding.\n",
    "    '''\n",
    "    color_vectors = {\n",
    "        'b': np.array([1, 0, 0, 0, 0, 0]),\n",
    "        'g': np.array([0, 1, 0, 0, 0, 0]),\n",
    "        'o': np.array([0, 0, 1, 0, 0, 0]),\n",
    "        'r': np.array([0, 0, 0, 1, 0, 0]),\n",
    "        'w': np.array([0, 0, 0, 0, 1, 0]),\n",
    "        'y': np.array([0, 0, 0, 0, 0, 1]),\n",
    "    }\n",
    "    state = np.zeros(shape=(6, 9, 6))\n",
    "    for i, face in enumerate('ULFRBD'):\n",
    "        unpacked_face = [str(x)[1] for x in np.array(cube.get_face(face)).flatten()]\n",
    "        for j, square in enumerate(unpacked_face):\n",
    "            state[i, j] = color_vectors[square]\n",
    "    return state.flatten()\n",
    "\n",
    "\n",
    "def load_sequences(filename, num_sequences=1000):\n",
    "    '''Load a train.dat file and transform it into a series of cube states to move sequences.\n",
    "    '''\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1, 'U2': 2,\n",
    "        'L': 3, 'L\\'': 4, 'L2': 5,\n",
    "        'F': 6, 'F\\'': 1, 'F2': 8,\n",
    "        'R': 9, 'R\\'': 1, 'R2': 11,\n",
    "        'B': 12, 'B\\'': 1, 'B2': 14,\n",
    "        'D': 15, 'D\\'': 1, 'D2': 17, '$': 18\n",
    "    }\n",
    "    with open(filename, 'r') as f:\n",
    "        print(f'OPENING FILE: {filename}')\n",
    "        log_i = 100\n",
    "        sequences = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i == num_sequences:\n",
    "                return sequences\n",
    "            if i % log_i == 0:\n",
    "                print(f'LINE: {i}')\n",
    "            string_state, solution = line.strip().split('|')\n",
    "            unsolved_cube = convert_string_state_to_cube(string_state)\n",
    "            sequence = []\n",
    "            for step in solution.split():\n",
    "                sequence.append((convert_cube_to_state(unsolved_cube), move_mapping[step]))\n",
    "                unsolved_cube.perform_step(step)\n",
    "            sequence.append((convert_cube_to_state(unsolved_cube), '$'))\n",
    "            sequences.append(sequence)\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubeRNN(nn.Module):\n",
    "    def __init__(self, num_pieces, embedding_dim, hidden_size, output_size, num_layers=1):\n",
    "        super(CubeRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_pieces, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "flattened_cube_state = torch.arange(27)\n",
    "\n",
    "hidden_size = 128\n",
    "output_size = 12 \n",
    "\n",
    "model = CubeRNN(27, 8, hidden_size, output_size)\n",
    "\n",
    "input_tensor = torch.Tensor(np.arange(27)).to(torch.int64)\n",
    "\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(\"Model Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we want to pull one line -- turn it into a sequence of cube states to moves -- then train on it -- then pull the next line and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = pc.Cube()\n",
    "\n",
    "alg = pc.Formula()\n",
    "random_alg = alg.random()\n",
    "cube(random_alg)\n",
    "\n",
    "index = 0\n",
    "for c in cube:\n",
    "    # piece_to_index_mapping[c[1]] = index\n",
    "    # location_to_array_position_mapping\n",
    "    print(c)\n",
    "    # index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do we even need centers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cube(cube, piece_to_index_mapping, location_to_array_position_mapping):\n",
    "    cube_array = [None for _ in range(26)]\n",
    "    for i, cubie_tuple in enumerate(cube):\n",
    "        location, cubie = cubie_tuple\n",
    "        squares = []\n",
    "        for square in cubie:\n",
    "            squares.append(str(square[1]))\n",
    "        squares = tuple(squares)\n",
    "        cube_array[location_to_array_position_mapping[location]] = piece_to_index_mapping[squares]\n",
    "    return torch.Tensor([c for c in cube_array]).to(torch.int64)\n",
    "\n",
    "def encode_move(move):\n",
    "    move_mapping = {\n",
    "        'U': 0, 'U\\'': 1,\n",
    "        'L': 2, 'L\\'': 3,\n",
    "        'F': 4, 'F\\'': 5,\n",
    "        'R': 6, 'R\\'': 7,\n",
    "        'B': 8, 'B\\'': 9,\n",
    "        'D': 10, 'D\\'': 11,\n",
    "        '$': 12\n",
    "    }\n",
    "    return move_mapping[move]\n",
    "\n",
    "def get_sequences(filename, num_sequences=1000):\n",
    "    log_sequences = 0\n",
    "    sequences = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(sequences) == num_sequences:\n",
    "                return sequences\n",
    "            if len(sequences) % log_sequences == 0:\n",
    "                print(f'Sequence {log_sequences} of {num_sequences}...')\n",
    "            cubies_string, solution_string = line.split('|')\n",
    "            solution_string.translate(str.maketrans({'U2': 'U U', 'L2': 'L L', 'R2': 'R R', 'F2': 'F F', 'D2': 'D D', 'B2': 'B B'}))\n",
    "            cubies_set = set([eval(cubie_string) for cubie_string in cubies_string])\n",
    "            cube = pc.Cube(cubies_set)\n",
    "            moves = solution_string.split()\n",
    "            cube_states = []\n",
    "            move_states = []\n",
    "            for move in moves:\n",
    "                cube_states.append(encode_cube(cube))\n",
    "                move_states.append(encode_move(move))\n",
    "                cube.perform_step(move)\n",
    "            cube_states.append(encode_cube(cube))\n",
    "            move_states.append(encode_move('$'))\n",
    "            sequences.append(cube_states, move_states)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index_mapping = {}\n",
    "location_to_array_position_mapping = {}\n",
    "\n",
    "cube = pc.Cube()\n",
    "index = 0\n",
    "for c in cube:\n",
    "    piece_to_index_mapping[tuple([str(square[1]) for square in c[1]])] = index\n",
    "    location_to_array_position_mapping[c[0]] = index\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_to_array_position_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuber.solver import CFOPSolver\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'CUBE {i}:')\n",
    "    cube = pc.Cube()\n",
    "    alg = pc.Formula()\n",
    "    random_alg = alg.random()\n",
    "    cube(random_alg)\n",
    "    original_cube = cube.copy()\n",
    "    print(f'SCRAMBLED:\\n{encode_cube(cube, piece_to_index_mapping, location_to_array_position_mapping)}')\n",
    "    print(cube)\n",
    "    print()\n",
    "    solver = CFOPSolver(cube)\n",
    "    solution = solver.solve(cube)\n",
    "    print(solution)\n",
    "    original_cube.perform_algo(solution)\n",
    "    print(f'SOLVED:\\n{encode_cube(original_cube, piece_to_index_mapping, location_to_array_position_mapping)}')\n",
    "    print(original_cube)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = get_sequences('data/train_0.dat', num_sequences=100)\n",
    "\n",
    "num_pieces = 27\n",
    "embedding_dim = 8\n",
    "hidden_size = 128\n",
    "output_size = 13\n",
    "num_layers = 1\n",
    "\n",
    "# Model\n",
    "model = CubeRNN(num_pieces, embedding_dim, hidden_size, output_size, num_layers)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubiks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
